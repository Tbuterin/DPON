import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import accelerate
import copy


def remove_trailing_value(tensor, value=-100):
    """
    åˆ é™¤å¼ é‡æœ«å°¾æ‰€æœ‰æŒ‡å®šå€¼çš„å…ƒç´ 
    
    :param tensor: è¾“å…¥å¼ é‡
    :param value: è¦åˆ é™¤çš„å€¼
    :return: åˆ é™¤æŒ‡å®šå€¼åçš„å¼ é‡
    """
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸ç­‰äº-100çš„å…ƒç´ ä½ç½®
    non_100_indices = (tensor != value).nonzero(as_tuple=True)[1]

    # å¦‚æœæœ‰ä¸ç­‰äº-100çš„å…ƒç´ ï¼Œåˆ™å–æœ€å¤§ç´¢å¼•åŠ 1ï¼ˆå› ä¸ºç´¢å¼•ä»0å¼€å§‹ï¼‰
    if len(non_100_indices) > 0:
        max_idx = non_100_indices[-1] + 1
        tensor = tensor[:, :max_idx]
    
    return tensor


class PPO():
    def __init__(self,
                 actor_critic,
                 optimizer,
                 accelerator,
                 clip_param,
                 ppo_epoch,
                 mini_batch_size,
                 value_loss_coef,
                 entropy_coef,
                 max_grad_norm=None,
                 use_clipped_value_loss=True):

        self.actor_critic = actor_critic

        self.mini_batch_size = mini_batch_size

        self.value_loss_coef = value_loss_coef
        self.entropy_coef = entropy_coef
        self.max_grad_norm = max_grad_norm
        self.clip_param = clip_param

        self.ppo_epoch = ppo_epoch

        self.use_clipped_value_loss = use_clipped_value_loss

        self.optimizer = optimizer
        self.accelerator = accelerator

    def update(self, rollouts):
        advantages = rollouts.returns[:-1] - rollouts.value_preds[:-1]
        advantages = (advantages - advantages.mean()) / (
            advantages.std() + 1e-5)

        value_loss_epoch = 0
        action_loss_epoch = 0
        dist_entropy_epoch = 0
        grad_step = 0
        self.actor_critic.train()
        for e in range(self.ppo_epoch):
            data_generator = rollouts.feed_forward_generator(
                    advantages, self.mini_batch_size)
            for sample in data_generator:
                with self.accelerator.accumulate(self.actor_critic):
                    grad_step += 1
                    obs_batch, output_ids_batch, actions_batch, \
                    value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, \
                            adv_targ = sample
                    # Reshape to do in a single forward pass for all steps
                    values, action_log_probs = self.actor_critic.evaluate_actions(
                        obs_batch, output_ids_batch)
                    #values and action_log_probs on two different devices!! because they come from two llava
                    if torch.isnan(action_log_probs).any():
                        continue
                    old_action_log_probs_batch = old_action_log_probs_batch.to(action_log_probs.device).view(-1)
                    adv_targ = adv_targ.to(action_log_probs.device)
                    value_preds_batch = value_preds_batch.to(values.device)
                    return_batch = return_batch.to(values.device)


                    ratio = torch.exp(action_log_probs -
                                    old_action_log_probs_batch)

                    surr1 = ratio * adv_targ
                    surr2 = torch.clamp(ratio, 1.0 - self.clip_param,
                                        1.0 + self.clip_param) * adv_targ
                    ## adding a ratio clip, inspired by https://github.com/huggingface/trl/blob/5a233546ee48532eaeb24b89b8d0042147574688/trl/trainer/ppo_trainer.py#L1199
                    if torch.any(ratio > 10):
                        action_loss = -surr2.mean()
                    else:
                        action_loss = -torch.min(surr1, surr2).mean()
                    if self.use_clipped_value_loss:
                        value_pred_clipped = value_preds_batch + \
                            (values - value_preds_batch).clamp(-self.clip_param, self.clip_param)
                        value_losses = (values - return_batch).pow(2)
                        value_losses_clipped = (
                            value_pred_clipped - return_batch).pow(2)
                        value_loss = 0.5 * torch.max(value_losses,
                                                    value_losses_clipped).mean()
                    else:
                        value_loss = 0.5 * (return_batch - values).pow(2).mean()

                    try:
                        assert not torch.isnan(value_loss), "value_loss is nan"
                        assert not torch.isnan(action_loss), "action_loss is nan"
                    except:
                        print("value/action loss is nan")
                        exit(1)
                    loss = value_loss * self.value_loss_coef+action_loss
                    self.accelerator.backward(loss)
                    if self.accelerator.sync_gradients:
                        self.accelerator.clip_grad_norm_(
                            self.actor_critic.parameters(),
                            self.max_grad_norm
                        )
                    self.optimizer.step()
                    self.optimizer.zero_grad()

                    value_loss_epoch += value_loss.item()
                    action_loss_epoch += action_loss.item()

        value_loss_epoch /= grad_step
        action_loss_epoch /= grad_step
        dist_entropy_epoch /= grad_step

        return value_loss_epoch, action_loss_epoch, dist_entropy_epoch



# jkc240830: DPO
class DPO():
    def __init__(self,
                 training_args,
                 policy_model,  # ä¿®æ”¹: åŸæ¥çš„actor_criticæ”¹ä¸ºpolicy_modelï¼Œè¡¨ç¤ºDPOä¸­çš„ç­–ç•¥æ¨¡å‹
                 reference_model,  # æ·»åŠ : å‚è€ƒæ¨¡å‹ï¼Œç”¨äºè®¡ç®—DPOæŸå¤±
                 optimizer,
                 accelerator,
                 beta,  # ä¿®æ”¹: PPOçš„clip_paramæ›¿æ¢ä¸ºDPOçš„betaå‚æ•°ï¼Œç”¨äºæ§åˆ¶DPOæŸå¤±ä¸­çš„æ¸©åº¦ç³»æ•°
                 dpo_epoch,  # ä¿®æ”¹: ppo_epochæ›¿æ¢ä¸ºdpo_epochï¼Œè¡¨ç¤ºDPOçš„è®­ç»ƒè½®æ•°
                 mini_batch_size,
                 max_grad_norm=None,
                 label_smoothing=0.0,  # æ·»åŠ : æ ‡ç­¾å¹³æ»‘å‚æ•°ï¼Œç”¨äºDPOæŸå¤±è®¡ç®—
                 ipo=False,
                 reference_free=False, # æ·»åŠ : æ˜¯å¦ä½¿ç”¨å‚è€ƒæ¨¡å‹çš„æ ‡å¿—
                 ref_regular=False,  # jkc0920
                 gamma=0.1,  # jkc0920
                 tknz = None,  # jkc0921
                 ):  

        self.policy_model = policy_model  # å°†actor_criticæ”¹ä¸ºpolicy_model: DPOPolicy
        self.reference_model = reference_model  # ä¿å­˜å‚è€ƒæ¨¡å‹ï¼ˆæ·»åŠ reference_modelå±æ€§ï¼‰
        self.mini_batch_size = mini_batch_size
        self.beta = beta  # å°†clip_paramæ›¿æ¢ä¸ºbeta
        self.dpo_epoch = dpo_epoch  # å°†ppo_epochæ›¿æ¢ä¸ºdpo_epoch
        self.label_smoothing = label_smoothing  # æ·»åŠ label_smoothingå±æ€§
        self.ipo = ipo
        self.reference_free = reference_free  # æ·»åŠ reference_freeå±æ€§

        self.optimizer = optimizer
        self.accelerator = accelerator
        self.max_grad_norm = max_grad_norm

        self.regular = ref_regular  # jkc0920
        self.gamma=gamma  # jkc0920

        self.tknz=tknz  # jkc0921

        self.training_args = training_args
        self.use_action_prob_w = training_args.use_action_prob_w

    def update(self, rollouts, check_grad=False):
        """
        è¿›è¡ŒDPOçš„å‚æ•°æ›´æ–°ã€‚
        rollouts: ç”±RolloutStorageç®¡ç†çš„æ”¶é›†çš„æ ·æœ¬ï¼ŒåŒ…æ‹¬æˆå¯¹çš„è¾ƒå¥½å’Œè¾ƒå·®çš„æ ·æœ¬ã€‚
        """
        action_loss_epoch = 0  # DPOä¸­ä¸éœ€è¦åŒºåˆ†valueå’Œaction lossï¼Œå› æ­¤ç§»é™¤action_loss_epochå’Œdist_entropy_epoch
        # action_loss_epoch = 0
        grad_step = 0
        self.policy_model.train()  # å°†actor_criticæ”¹ä¸ºpolicy_model
        if self.training_args.print_out_bug_story:
            print(f">>>>>>è¿™é‡Œå¼€å§‹updateäº†ï¼Œç»è®¡ç®—æœ‰æ•ˆæ ·æœ¬å¯¹å…±{rollouts.valid_pairs}å¯¹")
        for e in range(self.dpo_epoch):
            data_step = 0
            data_generator = rollouts.feed_forward_generator(self.mini_batch_size)  # @TODO
            for pre_obs, rej_obs, pre_prompt, rej_prompt, better_sample, worse_sample, pre_action_log_prob in data_generator:  # jkc0920
                # jkc0921
                pre_prompt = copy.deepcopy(remove_trailing_value(pre_prompt))
                rej_prompt = copy.deepcopy(remove_trailing_value(rej_prompt))

                if self.training_args.print_out_bug_story:
                    print(f"æ£€æŸ¥ä¸€ä¸‹winæ ·æœ¬çš„promptå’Œloseæ ·æœ¬çš„promptçš„æ•°æ®ç±»å‹: {type(pre_prompt)}, {type(rej_prompt)} \
                          æ•°æ®å¤§å°: {pre_prompt.shape}, {rej_prompt.shape}ã€‚")
                data_step += 1
                print(f"\033[35m{data_step} / {rollouts.valid_pairs}\033[0m")

                ######## åˆæ³•æ€§ ########
                non_zero_better_elements = torch.nonzero(better_sample)
                non_zero_worse_elements = torch.nonzero(worse_sample)

                if self.training_args.print_out_bug_story:
                    print(f"æ£€æŸ¥ä¸‹winåŠ¨ä½œå’ŒloseåŠ¨ä½œçš„æ•°æ®ç±»å‹: {type(better_sample)}, {type(worse_sample)} \
                          æ•°æ®å°ºå¯¸: {better_sample.shape}, {worse_sample.shape}, å°ºå¯¸çš„ç¬¬äºŒç»´åº¦åº”è¯¥ç­‰äº2 * max_new_tokens \
                          æ•°æ®ä¸­éé›¶å…ƒç´ çš„ä¸ªæ•°: {non_zero_better_elements.size(0)}, {non_zero_worse_elements.size(0)} \
                          ")
                # è®¡ç®—éé›¶å…ƒç´ çš„æ•°é‡
                num_non_zero_elements = max(non_zero_better_elements.size(0), non_zero_worse_elements.size(0))
                if num_non_zero_elements < 6:
                    print(f"\033[43mGrad Safe Problem!!!\033[0m")
                    continue
                ######## åˆæ³•æ€§ ########

                ######## loss backward debug ########
                if check_grad:
                    # print(f"\033[34m>>>OBS {obs.size()}, isnan: {torch.isnan(obs).any()}, detype: {obs.dtype}\033[0m")
                    print(f"\033[35m>>>PT {pre_prompt.size()}: {pre_prompt}, isnan: {torch.isnan(pre_prompt).any()}\033[0m")
                    print(f"\033[32m>>>good {better_sample.size()}: {better_sample}\033[0m")
                    print(f"\033[33m>>>bad {worse_sample.size()}: {worse_sample}\033[0m")
            
                with self.accelerator.accumulate(self.policy_model):  # å°†actor_criticæ›¿æ¢ä¸ºpolicy_model
                    grad_step += 1
                    better_obs_batch, better_output_ids_batch = pre_obs, better_sample
                    worse_obs_batch, worse_output_ids_batch = rej_obs, worse_sample
                    if self.training_args.print_out_bug_story:
                        print(f"å¼€å§‹æ¢¯åº¦åå‘ä¼ æ’­ã€‚\
                              æ£€æŸ¥better_obs_batch(å›¾åƒobs)ä»¥åŠworseçš„æ•°æ®ç±»å‹: {better_obs_batch.dtype},{worse_obs_batch.dtype}\n\
                              å°ºå¯¸: {better_obs_batch.shape}, {worse_obs_batch.shape} \
                              æ£€æŸ¥better_output_ids_batch(å’Œbetter_sampleä¸€è‡´, æ˜¯winçš„thts+actioné‡‡æ ·)ä»¥åŠworseçš„æ•°æ®ç±»å‹:{type(better_output_ids_batch)}, {type(worse_output_ids_batch)} \
                              æ•°æ®å°ºå¯¸: {better_output_ids_batch.shape}, {worse_output_ids_batch.shape}")
                    # è¯„ä¼°ç­–ç•¥æ¨¡å‹åœ¨è¾ƒå¥½å’Œè¾ƒå·®æ ·æœ¬ä¸Šçš„å¯¹æ•°æ¦‚ç‡
                    better_log_probs, better_action_log_probs = self.policy_model.evaluate_actions(better_obs_batch, better_output_ids_batch, INPUT_IDS=pre_prompt) # obsæ˜¯å›¾ç‰‡ï¼Œidsæ˜¯æ–‡æœ¬
                    worse_log_probs, worse_action_log_probs = self.policy_model.evaluate_actions(worse_obs_batch, worse_output_ids_batch, INPUT_IDS=rej_prompt)  # è¿™ä¸¤ä¸ªæ˜¯è¦åŠ input_IDSçš„
                    
                    if self.training_args.print_out_bug_story:
                        print(f"ç»è¿‡äº†policy_modelçš„evaluate_actions, è¾“å‡ºbetter_log_probså’Œworseå³åŠ æƒçš„thts&actionæ¦‚ç‡, \
                              æ•°æ®ç±»å‹: {better_log_probs.dtype}, {worse_log_probs.dtype} \n\
                              æ•°æ®å†…å®¹: {better_log_probs}, {worse_log_probs}, \n\
                              æ•°æ®æ¢¯åº¦: {better_log_probs.requires_grad}, {worse_log_probs.requires_grad} \n\
                              é¡ºå¸¦æ£€æŸ¥ä¸€ä¸‹åŠ¨ä½œçš„å•ç‹¬çš„æ¦‚ç‡: \n\
                              æ•°æ®ç±»å‹: {better_action_log_probs.dtype}, {worse_action_log_probs.dtype} \n\
                              æ•°æ®å†…å®¹: {better_action_log_probs}, {worse_action_log_probs}")
                    # if self.tknz is not None:
                    #     try: print(f"\033[46mbetter action text: {self.tknz.batch_decode(better_output_ids_batch, skip_special_tokens=True)}\033[0m")
                    #     except Exception as e: print(f"\033[31mbetter action text: {e}\033[0m")
                    #     try: print(f"\033[35mprompt: {self.tknz.batch_decode(prompt, skip_special_tokens=True)}\033[0m")
                    #     except Exception as e: print(f"\033[31mprompt: {e}\033[0m")
                    # print(f"\033[33moutput_id: {better_output_ids_batch.shape}: {better_output_ids_batch}\033[0m")
                    # print(f"\033[43mINPUT_IDS: {prompt.shape}: {better_output_ids_batch}\033[0m")

                    if check_grad:
                        policy_model_requires_grad = any(param.requires_grad for param in self.policy_model.parameters())
                        print(f"\033[43mpolicy_model in update requires grad: {policy_model_requires_grad}\033[0m")
                        # æ£€æŸ¥ log_probs çš„æ¢¯åº¦ä¿¡æ¯
                        print("\033[32mBetter log probs requires grad:", better_log_probs.requires_grad)
                        # print("\033[32mBetter log probs grad fn:", better_log_probs.grad_fn)
                        print("\033[32mWorse log probs requires grad:", worse_log_probs.requires_grad)
                        # print("\033[32mWorse log probs grad fn:", worse_log_probs.grad_fn)
                    
                    
                    # Forward pass for reference model (or use precomputed reference log probs)
                    with torch.no_grad():  # jkc0904
                        # print(f"\033[32mreference free: {self.reference_free}\033[0m")
                        if self.reference_free:
                            reference_chosen_logps = torch.zeros_like(better_log_probs)  # reference_freeæ¨¡å¼ä¸‹ï¼Œå‚è€ƒæ¨¡å‹çš„log probsä¸º0
                            reference_reject_logps = torch.zeros_like(worse_log_probs)
                        else:
                            reference_chosen_logps, better_action_log_probs = self.reference_model.evaluate_actions(better_obs_batch, better_output_ids_batch, INPUT_IDS=pre_prompt)
                            reference_reject_logps, worse_action_log_probs = self.reference_model.evaluate_actions(worse_obs_batch, worse_output_ids_batch, INPUT_IDS=rej_prompt)
                        
                        if self.training_args.print_out_bug_story:
                            print(f"è¿™é‡Œæ²¡æœ‰æ¢¯åº¦ï¼Œæ£€æŸ¥ref_modelçš„å€¼æ˜¯å¦æ­£ç¡®ã€‚\n\
                                  reference_chosen_logpsçš„æ•°æ®ç±»å‹:{reference_chosen_logps.dtype}, å°ºå¯¸: {reference_chosen_logps.shape}")

                    # print(f"\033[42mbetter_log_probs: {type(better_log_probs)}\nworse_log_probs: {type(worse_log_probs)}\n\
                    #       \033[44mpre_action_log_prob: {pre_action_log_prob}--{type(pre_action_log_prob)}\n\
                    #       \033[45mbetter_action_log_probs: {type(better_action_log_probs)}\n\
                    #       worse_action_log_probs: {type(worse_action_log_probs)}\033[0m")
                    # è®¡ç®—DPOæŸå¤±
                    if self.training_args.print_out_bug_story:
                        print(f">>>è®¡ç®—dpo_loss")
                    losses, chosen_rewards, rejected_rewards = self.dpo_loss(
                        better_log_probs,  # grad
                        worse_log_probs,  # grad
                        reference_chosen_logps,
                        reference_reject_logps,
                        self.beta,
                        self.label_smoothing,
                        self.gamma, # jkc0920
                        pre_action_log_prob,  # jkc0920
                        better_action_log_probs,  # jkc0920
                        worse_action_log_probs,  # jkc0921
                        self.ipo,
                        self.reference_free,
                    )

                    if self.training_args.print_out_bug_story:
                        print(f"\033[34mè‡³å…³é‡è¦çš„ä¸€æ­¥ï¼Œæ£€æŸ¥loss:{losses}, ç±»å‹{type(losses)}, ç»´åº¦{losses.shape}, æ¢¯åº¦{losses.requires_grad}\033[0m")

                    # æ£€æŸ¥æ•°æ®åˆæ³•æ€§
                    # print(f"\033[43mlosses is: {losses}, {torch.isnan(losses)}\033[0m")
                    try:
                        assert not torch.isnan(losses).any(), "loss contains nan"
                    except:
                        print("\033[31mloss contains nan\033[0m")
                        exit(1)

                    if check_grad:
                        print(f"\033[32mLosses requires grad: {losses.requires_grad}\033[0m")
                        print(f"\033[32mLosses grad_fn: {losses.grad_fn}\033[0m")

                    loss = losses.mean()

                    if self.training_args.print_out_bug_story:
                        print(f"losseså–meanä¹‹åçš„loss: {loss}, ç±»å‹: {loss.dtype}, ç»´åº¦: {loss.shape}, æ¢¯åº¦: {loss.requires_grad}")
                        print(f"æ£€æŸ¥ç»“æŸ<<<<<<")
                    if check_grad:
                        print(f"\033[32mLosses requires grad after mean: {losses.requires_grad}\033[0m")
                        print(f"\033[32mLosses grad_fn after mean: {losses.grad_fn}\033[0m")


                    # åå‘ä¼ æ’­å’Œæ¢¯åº¦æ›´æ–°
                    try:
                        self.accelerator.backward(loss)
                        if self.accelerator.sync_gradients:
                            self.accelerator.clip_grad_norm_(
                                self.policy_model.parameters(),
                                self.max_grad_norm
                            )
                        self.optimizer.step()
                        self.optimizer.zero_grad()

                        action_loss_epoch += loss.item()
                    except Exception as e:
                        for _ in range(2): print("\033[41m##############################!!!\033[0m")
                        print(f"\033[41m{e}: losses: {losses}, better_output_ids_batch: {better_output_ids_batch.shape}\n \
                              better_log_probs: {better_log_probs}\n \
                              better_action_log_probs: {better_action_log_probs}\033[0m")
                        for _ in range(2): print("\033[41m##############################!!!\033[0m")
        
        try:
            action_loss_epoch /= grad_step
            print(f"losses: {action_loss_epoch}, epoch: {e/self.dpo_epoch}")
        except Exception as e:
            print(f"\033[41m<<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!>>\033[0m")
            print(f"\033[41m{e}\033[0m")
            print(f"\033[41m<<!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!>>\033[0m")

        return action_loss_epoch

    def dpo_loss(self, 
                 policy_chosen_logps, 
                 policy_rejected_logps,
                 reference_chosen_logps, 
                 reference_rejected_logps,
                 beta, 
                 label_smoothing=0.0, 
                 gamma=0.1, 
                 ref_action_log_prob=torch.tensor([[0.]]), 
                 better_action_log_prob=torch.tensor([[0.]]), 
                 worse_action_log_prob=torch.tensor([[0.]]),
                 ipo=False, 
                 reference_free=False):
        """
        è®¡ç®—DPOæŸå¤±å‡½æ•°
        policy_chosen_logps: ç­–ç•¥æ¨¡å‹å¯¹é€‰æ‹©æ ·æœ¬çš„å¯¹æ•°æ¦‚ç‡
        policy_rejected_logps: ç­–ç•¥æ¨¡å‹å¯¹æœªé€‰æ‹©æ ·æœ¬çš„å¯¹æ•°æ¦‚ç‡
        reference_chosen_logps: å‚è€ƒæ¨¡å‹å¯¹é€‰æ‹©æ ·æœ¬çš„å¯¹æ•°æ¦‚ç‡
        reference_rejected_logps: å‚è€ƒæ¨¡å‹å¯¹æœªé€‰æ‹©æ ·æœ¬çš„å¯¹æ•°æ¦‚ç‡
        beta: DPOæŸå¤±çš„æ¸©åº¦å‚æ•°
        label_smoothing: DPOæŸå¤±çš„æ ‡ç­¾å¹³æ»‘å‚æ•°
        ipo: æ˜¯å¦ä½¿ç”¨IPOæŸå¤±
        reference_free: æ˜¯å¦å¿½ç•¥å‚è€ƒæ¨¡å‹
        """


        if self.reference_free:
            ref_logratios = 0  # å¦‚æœä¸ä½¿ç”¨å‚è€ƒæ¨¡å‹

        # jkc0922
        
        if self.use_action_prob_w:
            with torch.no_grad():
                action_chosen_ps = torch.exp(better_action_log_prob)   #ğŸŒŸjkc0924 
                action_rejected_ps = torch.exp(worse_action_log_prob)  #ğŸŒŸjkc0924
            # if torch.all(action_chosen_ps==0.) and torch.all(action_rejected_ps==0.):
            if torch.all(action_chosen_ps<0.3) and torch.all(action_rejected_ps<0.3):  # jkc0925
                pi_logratios = torch.tensor(0.2) * (- policy_chosen_logps - policy_rejected_logps)  # jkc0924
            else:
                pi_logratios = action_chosen_ps * policy_chosen_logps - action_rejected_ps * policy_rejected_logps

        else:
            pi_logratios = policy_chosen_logps - policy_rejected_logps
       

        # pi_logratios = action_chosen_ps * policy_chosen_logps - action_rejected_ps * policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps

        logits = pi_logratios - ref_logratios
        # print(f"\033[32m logits: {logits} \033[0m")
        # print(f"\033[42m{F.logsigmoid(logits)}\033[0m")

        # è®¡ç®—DPOæŸå¤±
        losses = -F.logsigmoid(beta * logits) * (1 - label_smoothing) - F.logsigmoid(-beta * logits) * label_smoothing
        # print(f"\033[31mlosses is: {losses}\033[0m")
        # print(f"è¿™é‡Œæ£€æŸ¥ä¸€ä¸‹lossesåœ¨dpo_losså‡½æ•°ä¸­è®¡ç®—å‡ºæ¥åçš„åˆæ³•æ€§:{losses}, ç±»å‹{losses.dtype}, å°ºå¯¸: {losses.shape}, logits: {logits}")
        if self.regular:
            # a=ref_action_log_prob.to(better_action_prob.device)
            # print(f"\033[42maction_log_prob: {ref_action_log_prob.device}\033[0m")
            # print(f"\033[42mpolicy_chosen: {better_action_prob.device}\033[0m")
            
            # print(f"\033[42m{a.device}\033[0m")
            # reg_losses = torch.exp(better_action_prob) * (better_action_prob - ref_action_log_prob.detach())   # jkc0920
            # reg_losses = -better_action_prob * (torch.exp(ref_action_log_prob.detach()) / torch.exp(better_action_prob))
            reg_losses = gamma * torch.exp(ref_action_log_prob.detach()) * nn.MSELoss()(better_action_log_prob, ref_action_log_prob.detach()).to(losses.dtype)  # jkc0924
            # print("\033[36m##################################################")
            # print(f"policy_chosen_logps: {better_action_prob}")
            # print(f"\033[31mpolicy_reject_logos: {worse_action_prob}\033[36m")
            # print(f"ref_origin: {ref_action_log_prob}")
            # # print(f"{better_action_prob - ref_action_log_prob.detach()}")
            # print(f"regular loss: {reg_losses}{reg_losses.shape}")
            # print("##################################################\033[0m")
            reg_losses_cliped = torch.clamp(reg_losses, min=-1.0, max=1.0).to(losses.dtype)  # jkc0923
            # print(f"æ£€æŸ¥ref_losses, å³klæ•£åº¦æ­£åˆ™é¡¹çš„åˆæ³•æ€§: {reg_losses_cliped}, ç±»å‹: {reg_losses_cliped.dtype}, å°ºå¯¸: {reg_losses_cliped.shape}")
            print(f"\033[45mloss_before: {losses}, {losses.dtype}\033[0m")
            # losses += reg_losses_cliped.view(1) * gamma  # jkc0920
            print(f"reg_loss: {reg_losses_cliped.dtype}")
            losses += reg_losses_cliped.view(1)  # jkc0924
            # print(f"åŠ ä¸Šæ­£åˆ™é¡¹å, æ£€æŸ¥lossesåˆæ³•æ€§: {losses}, ç±»å‹: {losses.dtype}, å°ºå¯¸: {losses.shape}")
            print(f"\033[45mloss_after: {losses}, {losses.dtype}\033[0m")
        # try:
        #     print(f"loss: {losses}, reg_loss_cliped: {reg_losses_cliped}, logits: {logits}, action_chosen_ps: {action_chosen_ps}, action_rejected_ps: {action_rejected_ps}, policy_chosen_logps: {policy_chosen_logps}, policy_rejected_logps: {policy_rejected_logps}")
        # except:
        #     try:
        #         print(f"loss: {losses}, logits: {logits}, action_chosen_ps: {action_chosen_ps}, action_rejected_ps: {action_rejected_ps}, policy_chosen_logps: {policy_chosen_logps}, policy_rejected_logps: {policy_rejected_logps}")
        #     except:
        #         try:
        #             print(f"loss: {losses}, logits: {logits}, policy_chosen_logps: {policy_chosen_logps}, policy_rejected_logps: {policy_rejected_logps}")
        #         except Exception as e:
        #             print(f"{e}")

        chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps).detach()
        rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps).detach()

        return losses, chosen_rewards, rejected_rewards

